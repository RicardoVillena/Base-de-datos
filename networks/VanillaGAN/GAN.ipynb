{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ubNyAhYB9pq"
      },
      "source": [
        "# Self Implementation of Vanilla GAN\n",
        "Paper: https://arxiv.org/pdf/1406.2661.pdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n"
      ],
      "metadata": {
        "id": "kHJGH708oV6I",
        "outputId": "408f0d84-8390-4e45-83f3-1e1a23315d9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1AcX1JwCPlo"
      },
      "source": [
        "array2gif allows making gifs from numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysQfJECB7Z5_",
        "outputId": "411f8240-de15-430a-9d54-87280feed4b9"
      },
      "source": [
        "!pip install array2gif"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting array2gif\n",
            "  Downloading array2gif-1.0.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from array2gif) (2.0.2)\n",
            "Downloading array2gif-1.0.4-py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: array2gif\n",
            "Successfully installed array2gif-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E69kk8i2rfXs"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import torchvision\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_size = 100):\n",
        "        super(Generator, self).__init__()\n",
        "        self.latent_size = latent_size\n",
        "        self.model       = nn.Sequential(nn.Linear(latent_size, 6*6),\n",
        "                                         nn.ReLU(),\n",
        "                                         nn.Linear(6*6, 12*12),\n",
        "                                         nn.ReLU(),\n",
        "                                         nn.Linear(12*12, 28*28),\n",
        "                                         nn.Tanh()\n",
        "                                         )\n",
        "    def __call__(self, latent):\n",
        "        return self.forward(latent)\n",
        "\n",
        "    def forward(self, latent):\n",
        "        gen = self.model(latent)\n",
        "        gen = gen.view((gen.size()[0],*(1,28,28)))\n",
        "        return gen\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(nn.Linear(28*28, 12*12),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Linear(12*12, 6*6),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Linear(6*6, 1),\n",
        "                              nn.Sigmoid()\n",
        "                              )\n",
        "    def __call__(self, img):\n",
        "        return self.forward(img)\n",
        "\n",
        "    def forward(self, img):\n",
        "        flattened = img.view(img.size()[0], -1)\n",
        "        decision = self.model(flattened.cuda())\n",
        "        return decision\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsBoSiSyCcSx"
      },
      "source": [
        "Function to plot results every now and then just for being sure everything is fine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUb_SAoZrjME"
      },
      "source": [
        "def plot_epoch(images, n=36):\n",
        "    '''\n",
        "    Visualize a single epoch of images\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    images : numpy.ndarray\n",
        "             images of shape (b, c, x, y)\n",
        "    n      : int, default: 36\n",
        "             number of images to display. Must be a square number\n",
        "    '''\n",
        "\n",
        "    if not isinstance(images, np.ndarray):\n",
        "        images = images.detach().numpy()\n",
        "\n",
        "    rowcols = np.sqrt(n)\n",
        "    plt.figure(figsize=(rowcols, rowcols))\n",
        "    for index in range(n):\n",
        "        plt.subplot(rowcols, rowcols, index + 1)\n",
        "        plt.imshow(images[index, 0, :, :], cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSJKjRCArueq",
        "outputId": "c0844ff0-22c6-4d7c-989c-08000af9f346"
      },
      "source": [
        "from torch.optim import Adam\n",
        "from torchvision.datasets import MNIST\n",
        "from array2gif import write_gif\n",
        "\n",
        "LATENT_SIZE = 100\n",
        "EPOCH_SIZE  = 300\n",
        "ROW_NUM     = 6\n",
        "try:\n",
        "  data_path = os.path.join(os.path.abspath(os.environ[\"CONDA_PREFIX\"]),\n",
        "                          'datasets')\n",
        "except KeyError:\n",
        "    data_path = os.path.join(os.path.abspath(os.environ[\"HOME\"]),\n",
        "                         'datasets')\n",
        "# We make sure that the dataset is actually available\n",
        "try:\n",
        "    torchvision.datasets.MNIST(root=data_path,\n",
        "                               download=False)\n",
        "except RuntimeError or KeyError:\n",
        "    if not os.path.isdir(data_path):\n",
        "            os.makedirs(data_path)\n",
        "    torchvision.datasets.MNIST(root=data_path,\n",
        "                               download=True)\n",
        "\n",
        "\n",
        "dataset = torchvision.datasets.MNIST(root=data_path,download=True)\n",
        "dataset.transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                                    torchvision.transforms.Normalize(mean = (0.5,),\n",
        "                                                                                     std = (0.5,))])\n",
        "\n",
        "\n",
        "# Data Loader\n",
        "batch_size = 128\n",
        "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True, drop_last=True)\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device is: \" + str(device))\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "discriminator = Discriminator().to(device)\n",
        "generator     = Generator().to(device)\n",
        "\n",
        "optim_d = Adam(discriminator.parameters(), lr = 0.0002)\n",
        "optim_g = Adam(generator.parameters(),     lr = 0.0002)\n",
        "\n",
        "gif_array = []*EPOCH_SIZE\n",
        "total_step = len(data_loader)\n",
        "for epoch in range(EPOCH_SIZE):\n",
        "    for step, (imgs, _) in enumerate(data_loader):\n",
        "\n",
        "        # Create Fake and Real Labels\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                      Train the discriminator                       #\n",
        "        # ================================================================== #\n",
        "        outputs     = discriminator(imgs)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "        real_score = outputs\n",
        "\n",
        "        # BCE on fake images\n",
        "        lat_space   = torch.randn(batch_size, LATENT_SIZE).to(device)\n",
        "        fake_images = generator(lat_space)\n",
        "        outputs     = discriminator(fake_images)\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "        fake_score  = outputs\n",
        "\n",
        "        # Backprop and optimize\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        optim_d.zero_grad()\n",
        "        optim_g.zero_grad()\n",
        "        d_loss.backward()\n",
        "        optim_d.step()\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Train the generator                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "        # Compute loss with fake images\n",
        "        lat_space = torch.randn(batch_size, LATENT_SIZE).to(device)\n",
        "        fake_images = generator(lat_space)\n",
        "        outputs = discriminator(fake_images)\n",
        "\n",
        "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        # Backprop and optimize\n",
        "        optim_d.zero_grad()\n",
        "        optim_g.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optim_g.step()\n",
        "\n",
        "        if (step+1) % 200 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}'\n",
        "                    .format(epoch, EPOCH_SIZE,\n",
        "                            step+1, total_step,\n",
        "                            d_loss.item(),\n",
        "                            g_loss.item(),\n",
        "                            real_score.mean().item(),\n",
        "                            fake_score.mean().item()))\n",
        "    # plot_epoch(fake_images.cpu())\n",
        "    array_2_make_grid = ((fake_images.cpu()[0:ROW_NUM**2, :, :, :] + 1) * (1/2) * 255).type(torch.uint8)\n",
        "    gif_array.append(torchvision.utils.make_grid(array_2_make_grid, nrow = ROW_NUM).numpy())\n",
        "write_gif(gif_array, filename = 'vanilla_10_fps.gif', fps = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 499kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.69MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 14.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is: cuda\n",
            "Epoch [0/300], Step [200/468], d_loss: 0.4350, g_loss: 1.4595, D(x): 0.91, D(G(z)): 0.28\n",
            "Epoch [0/300], Step [400/468], d_loss: 1.0629, g_loss: 0.7765, D(x): 0.77, D(G(z)): 0.53\n",
            "Epoch [1/300], Step [200/468], d_loss: 0.1727, g_loss: 2.5631, D(x): 0.95, D(G(z)): 0.11\n",
            "Epoch [1/300], Step [400/468], d_loss: 1.1835, g_loss: 1.6550, D(x): 0.66, D(G(z)): 0.44\n",
            "Epoch [2/300], Step [200/468], d_loss: 0.1483, g_loss: 3.3558, D(x): 0.93, D(G(z)): 0.06\n",
            "Epoch [2/300], Step [400/468], d_loss: 0.8825, g_loss: 1.3820, D(x): 0.76, D(G(z)): 0.37\n",
            "Epoch [3/300], Step [200/468], d_loss: 1.1328, g_loss: 1.5236, D(x): 0.58, D(G(z)): 0.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kAHSI03pkKH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}